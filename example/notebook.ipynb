{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b5ad312",
   "metadata": {},
   "source": [
    "# Got this working in Colab\n",
    "- need to adjust how modules are loaded for local use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98da0d87",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from det.distributions import NegativeBinomial\n",
    "from det.trainer import LossSlopeLRScheduler, Trainer\n",
    "\n",
    "import patsy\n",
    "import scanpy as sc\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "adata = sc.datasets.pbmc3k_processed()\n",
    "adata.layers[\"counts\"] = sc.datasets.pbmc3k()[adata.obs.index, adata.var.index].X.todense()\n",
    "\n",
    "X = patsy.dmatrix(\n",
    "    formula_like = \"0 + C(louvain)\",\n",
    "    data = adata.obs,\n",
    "    return_type = \"dataframe\"\n",
    ")\n",
    "\n",
    "y = pd.DataFrame(\n",
    "    adata.layers[\"counts\"],\n",
    "    index=adata.obs_names,\n",
    "    columns=adata.var_names\n",
    ")\n",
    "\n",
    "nb_model = NegativeBinomial()\n",
    "nb_model.setup(X, y)\n",
    "\n",
    "# — optimizer —\n",
    "optimizer = torch.optim.Adam(list(nb_model.parameters.values()), lr=1.0)\n",
    "\n",
    "# — optional ReduceLROnPlateau —\n",
    "plateau = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=5, threshold=1e-3\n",
    ")\n",
    "\n",
    "# — adaptive wrapper —\n",
    "sched = LossSlopeLRScheduler(\n",
    "    optimizer,\n",
    "    plateau_scheduler=plateau,   # pass None if you only want slope-based control\n",
    "    delta_low=1e-3,              # shrink LR when improvements are tiny\n",
    "    delta_high=1e-1,             # grow LR when improvements are big\n",
    "    decrease_factor=0.9,\n",
    "    increase_factor=1.1,\n",
    "    min_lr=1e-5,\n",
    "    max_lr=0.5,                  # cap at 0.5 since you said it's safe\n",
    "    warmup_steps=3,\n",
    "    cooldown_steps=2,\n",
    ")\n",
    "\n",
    "# — loss closure —\n",
    "def loss_fn():\n",
    "    return -nb_model.log_likelihood(X, y)\n",
    "\n",
    "# — train —\n",
    "Trainer(optimizer, sched, loss_fn).fit(epochs=10, show_tqdm=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
